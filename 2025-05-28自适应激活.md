# Wed May 28 2025 06:30:56


我需要做一个全新的研究, 研究内容是: 自适应基于CDF的概率变化激活函数


我们当前的做法是： 
$$x \to h \to Z \to Y$$

我们希望做的是： 
$$x \to h \to Z \to Y'$$

where $G$ 是目标局部分布CDF with e.g. params $\mu, \sigma$， $F_C$ 是因果表 $Z$ 的柯西分布CDF $\text{Cauchy}(\delta, \gamma)$:
$$y' = G^{-1}(F_C(y)) \\ x \to h \to \text{Activation}(\mu, \sigma)$$


语言上来解释就是 abduction $x$ 得到的子群体，在特定 target 经函数 $G^{-1} \circ F_C$ 变化下， 是线性的因果关系。



啊，未来我们还有更多的事情要做，比如说输出是很多个维度的， 并且不同维度世界并不是相互独立的， 此时此刻我们就可能需要用到 Rosenblatt 变换及其逆变换。


MCauchy 过一个线性变化以后会得到一个什么分布？

MCauchy with $diag(\gamma_1, \gamma_2, \cdots, \gamma_n)$ 的 Rosenblatt 变换是什么？


---

**因果约束：** 


所以我们有三大类关系：embedding $x -> h$, abduction $x \to Z, z\sim Z$, action $z \to y$

再加上一个标签的常数尺度变化: $y \to y'$, 我们希望 $G^{-1} \circ F_C$ with params $\mu, \sigma, \delta, \gamma$ 是关于 $h$ 的泛函梯度为 0 的, 从而关于输入 $x$ 的泛函梯度为 0. 所以激活函数 $G^{-1} \circ F_C$ 的函数形式不会随着 $h$ 的变化而变化。也就是说：

$$  \frac{ \partial (\frac{\partial y'}{\partial y} G^{-1} \circ F_C) }{\partial h} = 0$$

一个特例是参数满足 $\mu = \delta, \sigma = \gamma$ 并且$F, G$ 同为 Cauchy 分布时，此时 $G^{-1} \circ F_C$ 是恒等映射。




---

**可逆神经网络：**


一个标准的 RevNet 块接收两组输入特征图（或者说，将输入特征图分成两半），我们称之为 $x_1$ 和 $x_2$。它也产生两组输出特征图 $y_1$ 和 $y_2$。

**前向传播:**
    该块的计算分为两步：
    $y_1 = x_1 + F(x_2)$  --- (式1)
    $y_2 = x_2 + G(y_1)$  --- (式2)

*   $x_1, x_2$: 输入该可逆块的两部分特征图。
*   $y_1, y_2$: 输出该可逆块的两部分特征图。
*   $F, G$: 这两个函数是标准的、**非可逆的**神经网络模块。它们通常与标准 ResNet 中的残差函数类似，例如由一系列卷积层、批归一化（Batch Normalization）和激活函数（如 ReLU）组成。重要的是，$F$ 和 $G$ 本身并不需要是可逆的。它们的作用是对输入进行复杂的非线性变换。

**可逆计算：**
    $x_1 = y_1 - F(x_2)$  --- (式1)
    $x_2 = y_2 - G(y_1)$  --- (式2)

你不需要担心因为 $F$ 和 $G$ 是什么“特殊”的可逆网络而导致表达能力受限。它们使用的是我们熟悉的、强大的标准神经网络组件。可逆性是一种巧妙的“封装”技巧，而不是对 $F$ 和 $G$ 内部结构的限制。


$$(x_1, x_2) \to (h_1^{(1)}, h_2^{(1)}) \to (h_1^{(2)}, h_2^{(2)}) \to \cdots \to (h_1^{(n)}, h_2^{(n)})$$

两层网络， 特例： 输入 $x_2 = 0$ 时， 

$$h_1^{(1)} = x_1 + F(x_2) = x_1 + F(0)$$

$h_2^{(1)} = x_2 + G(y_1) = G(x_1 + F(0))$ 之继续计算：

$$h_1^{(2)} = h_1^{(1)} + F(h_2^{(1)}) = x_1 + F(0) + F(G(x_1 + F(0)))$$

于是 $h_2^{(2)} = h_1^{(2)} + G(h_2^{(1)}) = x_1 + F(0) + F(G(x_1 + F(0))) + G(G(x_1 + F(0)))$



继续计算， 一直都是是可逆的。我们相当于使用辅助分量构建了一个可逆的神经网络







**我们处理高维的分布的时候, 需要 RevNet**


--- 


**我们是否需要 RevNet 来对结果进行变换？**

**不一定**, 恒等标签变化时， 我们输出的是一个柯西混合分布。

柯西混合分布是一个非常广泛和灵活的分布类，能够建模多峰、重尾、偏斜等多种复杂特征。

理论上，通过足够多的柯西混合成分，可以以任意精度逼近一个非常广泛的连续概率密度函数集合。 这意味着它们具有某种形式的“万能逼近”能力，尤其擅长处理重尾现象。

MLP-cauchy 就是这个一种网络，我们的CAAR 也是（同时有因果表征）

