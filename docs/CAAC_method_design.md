# CAAC (Cauchy Abduction Action Classification) 方法设计文档

## 1. 背景与动机

在成功构建了用于回归任务的 CAAR (Cauchy Abduction Action Regression) 方法后，我们现在着手设计一个统一的因果大模型架构，能够同时处理分类和回归任务。为此我们先单独处理分类任务， CAAC 旨在成为这个架构中处理分类任务的核心组件。

我们的核心目标是：
- 利用柯西分布的重尾特性来建模潜在变量，提高模型对异常值的鲁棒性
- 构建一个具有封闭解析NLL（负对数似然）的模型，便于梯度优化
- 避免采样过程，保证训练和推理的确定性
- 保持模型的因果解释性
- **关键约束**：输入数据 $x$ 仅应影响因果表征的生成，而因果表征到分类结果的映射机制应该是固定的、不依赖于特定输入的

## 2. 方案演进过程

### 2.1 初始多层柯西架构的探索与问题

**初始设想**：通过多层嵌套的柯西分布和 ALR (Additive Log-Ratio) 变换来构建分类模型：

$$x \rightarrow h(x) \rightarrow P(C|x) \sim \text{MCauchy}(\mu_C(h), \gamma_C(h))$$

其中包含高维因果表征 $C$、噪声 $\varepsilon$、线性变换和最终的ALR变换到分类概率。

**数学障碍**：为了获得封闭的NLL，需要计算：

$$P(Y=j|x) = \mathbb{E}_{C|x} \left[ \mathbb{E}_{\varepsilon|x} \left[ \mathbb{E}_{Z_1...Z_{K-1} | V(C,\varepsilon)} \left[ \text{ALR}_j(Z_1...Z_{K-1}) \right] \right] \right]$$

这种多层期望计算涉及没有初等函数封闭解的积分（如柯西分布与sigmoid函数的积分），无法满足"封闭解析NLL"的核心要求。

**结论**：初始的多层柯西架构在数学上不可行。

### 2.2 基础动态阈值方案：突破封闭解难题

**核心思想**：将问题简化为基于柯西分布的一维得分和分段分类：

$$x \rightarrow h(x) \rightarrow \begin{cases}
\mu_s(h), \gamma_s(h) & \text{(柯西得分参数)} \\
\theta_1(h), ..., \theta_{K-1}(h) & \text{(动态阈值)}
\end{cases}$$

**数学优势**：利用柯西CDF的封闭形式：
$$F_S(s; \mu, \gamma) = \frac{1}{2} + \frac{1}{\pi} \arctan\left(\frac{s - \mu}{\gamma}\right)$$

分类概率变为：
$$P(Y=k|x) = F_S(\theta_k(h)) - F_S(\theta_{k-1}(h))$$

**成就**：成功获得封闭解析NLL，但存在两个关键问题：
1. 缺乏显式的高维因果表征层
2. **决策阈值依赖于输入数据**，违背机制不变性原则

### 2.3 进阶动态阈值方案：引入显式因果表征

**设计改进**：引入显式的 $d_c$ 维独立柯西因果表征层：

$$x \rightarrow h(x) \rightarrow (\vec{\mu}_C(h), \vec{\gamma}_C(h)) \rightarrow \begin{cases}
\mu_s(h), \gamma_s(h) & \text{(通过线性组合)} \\
\theta_k(h) & \text{(通过小型神经网络)}
\end{cases}$$

具体地：
- $\mu_s(h) = \vec{\mu}_C(h)^T \vec{W}_\mu + b_\mu$
- $\gamma_s(h) = \sum_{i=1}^{d_c} |W_{\gamma,i}| \gamma_{C_i}(h) + \gamma_{\epsilon_0}$
- $\theta_k(h)$ 由 $\vec{\mu}_C(h)$ 通过MLP生成

**优势**：
- 显式高维因果表征层，更符合统一因果模型的目标
- 结构化的参数生成逻辑
- 保持封闭解析NLL

**核心问题仍未解决**：分类阈值 $\theta_k(h)$ 仍然依赖于输入数据，这与"因果表征一旦形成，其到结果的映射机制应更通用"的直觉相冲突。

## 3. CAAC-SPSFT：最终解决方案

### 3.1 核心设计理念

CAAC-SPSFT (Stochastic Pathway Selection with Fixed Thresholds) 方案彻底解决了机制不变性问题：

- **数据影响局限化**：输入 $x$ 仅影响因果表征 $C$ 的参数生成
- **机制固定化**：从因果表征到分类结果的所有决策机制都是全局固定的
- **多路径混合**：通过 $K$ 个并行的"解读路径"增强模型表达能力

### 3.2 完整数学架构

#### 3.2.1 输入到因果表征
$$x \rightarrow h(x) \rightarrow (\vec{\mu}_C(h), \vec{\gamma}_C(h))$$

其中 $\vec{\mu}_C(h) = (\mu_{C_1}(h), ..., \mu_{C_{d_c}}(h))$ 和 $\vec{\gamma}_C(h) = (\gamma_{C_1}(h), ..., \gamma_{C_{d_c}}(h))$ 定义了 $d_c$ 维独立柯西因果表征的分布参数。

#### 3.2.2 因果表征到多路径得分

对于每条路径 $j \in \{1, ..., K\}$，通过线性变换生成路径特定的柯西得分参数：

**位置参数**：
$$\mu_{S_j}(h) = (\vec{W}_{\mu}^{(j)})^T \vec{\mu}_C(h) + b_{\mu}^{(j)}$$

**尺度参数**：
$$\gamma_{S_j}(h) = \sum_{i=1}^{d_c} |W_{\gamma,i}^{(j)}| \gamma_{C_i}(h) + \gamma_{\epsilon_0}^{(j)}$$

其中 $\gamma_{\epsilon_0}^{(j)} = \exp(\text{raw\_}\gamma_{\epsilon_0}^{(j)})$ 确保正性。

#### 3.2.3 固定决策机制

**路径选择概率**（全局固定）：
$$\pi_j = \frac{\exp(l_j)}{\sum_{p=1}^K \exp(l_p)}$$

**分类阈值**（全局固定）：
$$\vec{\theta}^* = (\theta_1^* < \theta_2^* < ... < \theta_{K-1}^*)$$

#### 3.2.4 最终分类概率

每条路径 $j$ 给出的类别 $k$ 概率：
$$P(Y=k|M=j, x) = F_{S_j}(\theta_k^*) - F_{S_j}(\theta_{k-1}^*)$$

其中 $F_{S_j}$ 是柯西CDF：
$$F_{S_j}(s) = \frac{1}{2} + \frac{1}{\pi} \arctan\left(\frac{s - \mu_{S_j}(h)}{\gamma_{S_j}(h)}\right)$$

最终混合概率：
$$P(Y=k|x) = \sum_{j=1}^K \pi_j \cdot P(Y=k|M=j, x)$$

### 3.3 方案对比总结

| 方案 | 因果表征 | 决策机制 | 封闭NLL | 机制不变性 |
|------|----------|----------|---------|------------|
| 初始多层柯西 | ✓ 高维 | 复杂嵌套 | ✗ | N/A |
| 基础动态阈值 | ✗ 缺失 | 动态阈值 | ✓ | ✗ |
| 进阶动态阈值 | ✓ 高维 | 动态阈值 | ✓ | ✗ |
| **CAAC-SPSFT** | ✓ 高维 | **固定机制** | ✓ | ✓ |

## 4. CAAC-SPSFT 参数设计深入讨论

### 4.1 因果表征维度 $d_c$ 的选择

**设计考量**：
- **表达能力 vs 计算复杂度**：$d_c$ 决定了因果表征的丰富程度，但也直接影响后续每条路径的参数量
- **与任务复杂度的匹配**：对于 $K$ 类分类任务，$d_c$ 应该有足够的维度来捕获类别间的区别性信息
- **与编码器输出维度的关系**：$h(x)$ 的维度应该能够有效支撑 $2d_c$ 个输出（$\vec{\mu}_C$ 和 $\vec{\gamma}_C$）

**经验准则**：
- 起始点：$d_c = \lceil \log_2(K) \rceil \times 4$ 到 $d_c = K$
- 上界：避免 $d_c > K \times 10$，防止过度参数化
- 下界：确保 $d_c \geq \max(3, \lceil \log_2(K) \rceil)$，保证基本表达能力

### 4.2 路径数量 $K$ 的设计

**主要选择**：
1. **$K =$ 类别数量**：最直观的选择，每个路径对应一个类别的"视角"
2. **$K > $ 类别数量**：增加表达能力，但显著增加参数量
3. **$K < $ 类别数量**：减少复杂度，但可能损失表达能力

**权衡分析**：
- **参数量影响**：总参数量约为 $K \times (2d_c + 1)$（线性变换参数）
- **训练稳定性**：$K$ 过大可能导致路径间竞争，训练不稳定
- **可解释性**：$K =$ 类别数量时，每条路径可能自然对应某种类别特异的"解读方式"

### 4.3 线性变换参数的初始化策略

#### 4.3.1 位置参数权重 $\vec{W}_{\mu}^{(j)}$ 和偏置 $b_{\mu}^{(j)}$

**初始化原则**：
- **避免路径塌陷**：确保不同路径 $j$ 的 $\vec{W}_{\mu}^{(j)}$ 有足够的差异性
- **平衡性**：避免某些路径在初始化时就占据主导地位

**具体策略**：
1. **正交初始化**：当 $K \leq d_c$ 时，可以使用正交矩阵的行作为 $\vec{W}_{\mu}^{(j)}$
2. **随机扰动**：$\vec{W}_{\mu}^{(j)} \sim \mathcal{N}(0, \sigma^2/d_c)$，其中 $\sigma \approx 1.0$
3. **偏置设置**：$b_{\mu}^{(j)} = (j-1) \times \Delta$，其中 $\Delta$ 是一个小的间隔（如 $0.5$），确保初始时不同路径有不同的"基线"

#### 4.3.2 尺度参数权重 $\vec{W}_{\gamma}^{(j)}$ 和 $\gamma_{\epsilon_0}^{(j)}$

**设计目标**：
- **尺度参数的物理意义**：$\gamma_{S_j}(h)$ 表示路径 $j$ 下得分的"不确定性"
- **避免过大或过小**：防止数值不稳定或信息丢失

**初始化策略**：
1. **权重初始化**：$W_{\gamma,i}^{(j)} \sim \text{Uniform}(-0.1, 0.1)$，初始时让因果表征的贡献较小
2. **基础尺度**：$\text{raw\_}\gamma_{\epsilon_0}^{(j)} \sim \mathcal{N}(\log(1.0), 0.1^2)$，对应 $\gamma_{\epsilon_0}^{(j)} \approx 1.0$
3. **路径差异化**：不同路径 $j$ 可以有轻微不同的初始基础尺度

### 4.4 固定阈值 $\vec{\theta}^*$ 的设计

#### 4.4.1 阈值数量与位置

**基本约束**：$\theta_1^* < \theta_2^* < ... < \theta_{K-1}^*$

**初始化策略**：
1. **均匀分布**：$\theta_k^* = \Phi^{-1}(\frac{k}{K})$，其中 $\Phi^{-1}$ 是标准Cauchy分布的逆CDF
2. **基于先验**：如果有类别分布的先验知识，可以设置为累积概率的分位点
3. **对称分布**：$\theta_k^* = (k - \frac{K}{2}) \times \Delta$，其中 $\Delta \approx 1.0$

#### 4.4.2 阈值的参数化、学习与初始化

为了让这些阈值 $\theta_k^*$ 能够通过梯度下降进行学习，同时还要保证它们始终满足 $\theta_1^* < \theta_2^* < ... < \theta_{N_{cl}-1}^*$ 的顺序约束（其中 $N_{cl}$ 是类别数量，对应正文中使用的 $K$ 作为类别数时的场景，在此为避免与路径数量 $K_{paths}$ 混淆，我们明确为 $N_{cl}$ 个类别，需要 $N_{cl}-1$ 个阈值），我们不直接学习 $\theta_k^*$ 本身，而是学习一组更容易处理的原始参数（raw parameters）。

**1. 可学习的原始参数 (Learnable Raw Parameters):**

我们定义以下 $N_{cl}-1$ 个原始参数作为模型中实际学习的对象：
*   第一个阈值的原始参数：$\text{raw\_}\theta_1^*$
*   后续阈值与前一个阈值之间正差值的原始参数：$\{\text{raw\_}\delta_2^*, \text{raw\_}\delta_3^*, ..., \text{raw\_}\delta_{N_{cl}-1}^*\}$

总共有 $1 + (N_{cl}-2) = N_{cl}-1$ 个可学习的原始参数。

**2. 从原始参数计算实际使用的阈值 $\theta_k^*$:**

实际在模型中使用的阈值 $\theta_k^*$ 是通过下面的方式从这些可学习的原始参数计算得到的：

*   **第一个阈值 $\theta_1^*$**:
    $$\theta_1^* = \text{raw\_}\theta_1^*$$
    它直接由其原始参数决定。

*   **后续阈值 $\theta_k^*$ (对于 $k = 2, ..., N_{cl}-1$):**
    首先，我们计算阈值之间的正差值 $\delta_k^*$。为了确保这些差值是正的，我们对相应的原始参数 $\text{raw\_}\delta_k^*$ 应用一个保证输出为正的函数，例如 `exp` 或 `softplus`：
    $$\delta_k^* = \exp(\text{raw\_}\delta_k^*) \quad \text{或} \quad \delta_k^* = \text{softplus}(\text{raw\_}\delta_k^*)$$
    然后，后续的阈值通过累加这些正差值得到：
    $$\theta_2^* = \theta_1^* + \delta_2^*$$
    $$\theta_3^* = \theta_2^* + \delta_3^* = \theta_1^* + \delta_2^* + \delta_3^*$$
    $$\vdots$$
    $$\theta_k^* = \theta_{k-1}^* + \delta_k^* = \text{raw\_}\theta_1^* + \sum_{i=2}^{k} \delta_i^*$$

    这种参数化方式确保了只要 $\delta_i^*$ 是正的（通过 $\exp$ 或 $\text{softplus}$ 函数保证），那么 $\theta_1^* < \theta_2^* < ... < \theta_{N_{cl}-1}^*$ 的顺序就会被严格保持。

**3. 原始参数的初始化:**

初始化这些原始参数的目的是让它们在训练开始时就能产生一组合理的初始阈值 $\theta_k^{*(init)}$。这些初始阈值可以根据 "4.4.1 阈值数量与位置" 中提到的策略（如均匀分布或对称分布）来设定。

假设我们已经有了一组期望的初始阈值 $\theta_1^{*(init)}, \theta_2^{*(init)}, ..., \theta_{N_{cl}-1}^{*(init)}$，并且它们满足 $\theta_1^{*(init)} < \theta_2^{*(init)} < ...$。

*   **初始化 $\text{raw\_}\theta_1^*$**:
    $$\text{raw\_}\theta_1^{*(init)} = \theta_1^{*(init)}$$

*   **初始化 $\text{raw\_}\delta_k^*$ (对于 $k = 2, ..., N_{cl}-1$):**
    首先计算出期望的初始差值：
    $$\delta_k^{*(init)} = \theta_k^{*(init)} - \theta_{k-1}^{*(init)}$$
    由于我们在初始化时确保了 $\theta_k^{*(init)} > \theta_{k-1}^{*(init)}$，所以 $\delta_k^{*(init)}$ 必然为正。
    然后，我们通过所选正值函数的逆运算来得到 $\text{raw\_}\delta_k^{*(init)}$：
    *   如果使用 $\delta_k^* = \exp(\text{raw\_}\delta_k^*)$，那么：
        $$\text{raw\_}\delta_k^{*(init)} = \log(\delta_k^{*(init)}) = \log(\theta_k^{*(init)} - \theta_{k-1}^{*(init)})$$
    *   如果使用 $\delta_k^* = \text{softplus}(\text{raw\_}\delta_k^*)$，那么：
        $$\text{raw\_}\delta_k^{*(init)} = \text{softplus}^{-1}(\delta_k^{*(init)}) = \log(\exp(\delta_k^{*(init)}) - 1)$$

**4. 学习率调整:**

这些阈值相关的可学习参数（即 $\text{raw\_}\theta_1^*$ 和 $\text{raw\_}\delta_k^*$）通常需要与网络中的其他权重（如用于生成因果表征的编码器权重）使用不同的学习率。经验上，它们可能需要更小的学习率以保证训练的稳定性：
*   建议使用 $\text{lr}_{	heta} = \alpha \times \text{lr}_{\text{main}}$，其中 $\alpha$ 通常是一个较小的值（例如原文建议的 $0.1$）。

### 4.5 路径选择概率 $\vec{\pi}$ 的设计

#### 4.5.1 初始化策略

**平衡初始化**：
- $l_j = 0$ for all $j$，对应 $\pi_j = \frac{1}{K}$（均匀分布）
- 或者基于类别先验设置不均匀的初始分布

**多样性鼓励**：
- 在训练早期可以添加熵正则化：$-\lambda \sum_j \pi_j \log \pi_j$
- 防止单一路径主导的问题

#### 4.5.2 路径专门化机制

**自然专门化期望**：
- 理想情况下，不同路径 $j$ 应该自发地专门化处理某些类型的输入
- 路径选择概率 $\pi_j$ 反映了该路径的"重要性"或"适用范围"

**监控指标**：
- **路径利用率**：$\text{effective\_paths} = \exp(-\sum_j \pi_j \log \pi_j)$
- **路径特异性**：分析每条路径在不同类别上的表现差异

### 4.6 超参数调优的优先级

**第一优先级**（对性能影响最大）：
1. 因果表征维度 $d_c$
2. 路径数量 $K$
3. 基础尺度参数 $\gamma_{\epsilon_0}^{(j)}$ 的初始值

**第二优先级**（对训练稳定性影响大）：
1. 阈值的初始位置和学习率
2. 位置参数权重的初始化方式
3. 路径选择概率的正则化强度

**第三优先级**（精细调优）：
1. 尺度参数权重的初始化
2. 偏置项的设置
3. 不同参数组的学习率比例

### 4.7 潜在的设计变体

#### 4.7.1 自适应路径数量

**动态路径修剪**：
- 训练过程中监控 $\pi_j$，自动移除贡献很小的路径
- 实现方式：当 $\pi_j < \epsilon$ 持续多个epoch时，固定该路径的参数

#### 4.7.2 层次化路径结构

**分层路径选择**：
- 首先选择路径组，然后在组内选择具体路径
- 可能有助于处理具有层次结构的分类任务

#### 4.7.3 条件化阈值

**部分条件化**：虽然完全固定的阈值是我们的目标，但可以考虑基于全局统计量（而非特定输入）的微调：
$$\theta_k^* = \theta_{k,\text{base}}^* + \alpha \cdot \text{global\_shift}$$

其中 $\text{global\_shift}$ 是基于整个数据集统计的全局调整。

## 5. 总结与展望

CAAC-SPSFT 方案通过渐进式的设计改进，最终实现了：
1. **保持封闭解析NLL**：从基础动态阈值方案继承的核心优势
2. **显式因果表征**：从进阶动态阈值方案继承的结构完整性
3. **机制不变性**：通过固定决策机制彻底解决的核心问题

这个演进过程展示了在复杂约束下进行模型设计的思考路径：从数学可行性出发，逐步完善模型结构，最终在多个设计目标间找到最优平衡。

**下一步工作重点**：
1. **实验验证**：在真实分类任务上验证不同参数设计选择的有效性
2. **理论分析**：进一步分析混合柯西分布的表达能力边界
3. **与CAAR统一**：探索将CAAC-SPSFT与回归任务的CAAR方法统一到同一框架中

## 6. 附录：CAAC-SPSFT 在特定分类任务上的详细推导

本附录旨在详细阐述 CAAC-SPSFT 模型在常见的二分类和三分类任务中的具体数学表达。为清晰起见，我们首先定义：
- $N_{cl}$: 分类任务中的类别数量。
- $K_{paths}$: 模型中并行的"解读路径"数量 (在正文部分有时用 $K$ 指代)。
- $\vec{\theta}^* = (\theta_1^* < \theta_2^* < ... < \theta_{N_{cl}-1}^*)$: 全局固定的分类阈值。我们额外定义 $\theta_0^* = -\infty$ 和 $\theta_{N_{cl}}^* = +\infty$。

模型的核心组件如因果表征 $C$ 的参数 $(\vec{\mu}_C(h), \vec{\gamma}_C(h))$ 的生成、每条路径 $j$ 的柯西得分参数 $(\mu_{S_j}(h), \gamma_{S_j}(h))$ 的计算，以及路径选择概率 $\pi_j$ 的计算方式，均与正文 3.2 节所述保持一致。柯西分布的累积分布函数 (CDF) $F_{S_j}(s)$ 定义为：
$$F_{S_j}(s) = \frac{1}{2} + \frac{1}{\pi} \arctan\left(\frac{s - \mu_{S_j}(h)}{\gamma_{S_j}(h)}\right)$$

### 6.1 二分类 (Binary Classification, $N_{cl}=2$)

#### 6.1.1 模型设定
- **类别**: $C_1, C_2$ (例如，对应标签 $Y=1, Y=2$)
- **固定阈值**: $N_{cl}-1 = 1$ 个阈值，记为 $\theta_1^*$。
- **辅助阈值**: $\theta_0^* = -\infty, \theta_2^* = +\infty$。
- **解读路径数量**: $K_{paths}$

#### 6.1.2 单路径分类概率
对于任意一条解读路径 $j \in \{1, ..., K_{paths}\}$：
- 类别 $C_1$ 的概率:
  $$P(Y=C_1 | M=j, x) = F_{S_j}(\theta_1^*) - F_{S_j}(\theta_0^*) = F_{S_j}(\theta_1^*)$$
- 类别 $C_2$ 的概率:
  $$P(Y=C_2 | M=j, x) = F_{S_j}(\theta_2^*) - F_{S_j}(\theta_1^*) = 1 - F_{S_j}(\theta_1^*)$$

#### 6.1.3 整体分类概率
最终的分类概率通过对所有路径的概率进行加权平均得到：
- 类别 $C_1$ 的概率:
  $$P(Y=C_1 | x) = \sum_{j=1}^{K_{paths}} \pi_j \cdot P(Y=C_1 | M=j, x) = \sum_{j=1}^{K_{paths}} \pi_j \cdot F_{S_j}(\theta_1^*)$$
- 类别 $C_2$ 的概率:
  $$P(Y=C_2 | x) = \sum_{j=1}^{K_{paths}} \pi_j \cdot P(Y=C_2 | M=j, x) = \sum_{j=1}^{K_{paths}} \pi_j \cdot (1 - F_{S_j}(\theta_1^*))$$

### 6.2 三分类 (3-Class Classification, $N_{cl}=3$)

#### 6.2.1 模型设定
- **类别**: $C_1, C_2, C_3$ (例如，对应标签 $Y=1, Y=2, Y=3$)
- **固定阈值**: $N_{cl}-1 = 2$ 个阈值，记为 $\theta_1^*, \theta_2^*$，且满足 $\theta_1^* < \theta_2^*$。
- **辅助阈值**: $\theta_0^* = -\infty, \theta_3^* = +\infty$。
- **解读路径数量**: $K_{paths}$

#### 6.2.2 单路径分类概率
对于任意一条解读路径 $j \in \{1, ..., K_{paths}\}$：
- 类别 $C_1$ 的概率:
  $$P(Y=C_1 | M=j, x) = F_{S_j}(\theta_1^*) - F_{S_j}(\theta_0^*) = F_{S_j}(\theta_1^*)$$
- 类别 $C_2$ 的概率:
  $$P(Y=C_2 | M=j, x) = F_{S_j}(\theta_2^*) - F_{S_j}(\theta_1^*)$$
- 类别 $C_3$ 的概率:
  $$P(Y=C_3 | M=j, x) = F_{S_j}(\theta_3^*) - F_{S_j}(\theta_2^*) = 1 - F_{S_j}(\theta_2^*)$$

#### 6.2.3 整体分类概率
最终的分类概率通过对所有路径的概率进行加权平均得到：
- 类别 $C_1$ 的概率:
  $$P(Y=C_1 | x) = \sum_{j=1}^{K_{paths}} \pi_j \cdot P(Y=C_1 | M=j, x) = \sum_{j=1}^{K_{paths}} \pi_j \cdot F_{S_j}(\theta_1^*)$$
- 类别 $C_2$ 的概率:
  $$P(Y=C_2 | x) = \sum_{j=1}^{K_{paths}} \pi_j \cdot P(Y=C_2 | M=j, x) = \sum_{j=1}^{K_{paths}} \pi_j \cdot (F_{S_j}(\theta_2^*) - F_{S_j}(\theta_1^*))$$
- 类别 $C_3$ 的概率:
  $$P(Y=C_3 | x) = \sum_{j=1}^{K_{paths}} \pi_j \cdot P(Y=C_3 | M=j, x) = \sum_{j=1}^{K_{paths}} \pi_j \cdot (1 - F_{S_j}(\theta_2^*))$$

### 6.3 负对数似然 (NLL) 损失函数

对于一个包含 $N$ 个样本的数据集 $\{(x_i, y_i)\}_{i=1}^N$，其中 $x_i$ 是输入特征，$y_i$ 是真实的类别标签 (假设 $y_i \in \{C_1, ..., C_{N_{cl}}\}$)。模型的负对数似然损失函数定义为：

$$NLL = - \sum_{i=1}^N \log P(Y=y_i | x_i)$$

如果类别标签 $y_i$ 采用 one-hot 编码形式，例如 $y_i = (y_{i,1}, ..., y_{i,N_{cl}})$ 其中 $y_{i,c}=1$ 表示样本 $i$ 属于类别 $C_c$，否则为0，则损失函数可以写为：

$$NLL = - \sum_{i=1}^N \sum_{c=1}^{N_{cl}} y_{i,c} \log P(Y=C_c | x_i)$$

模型训练的目标是最小化此 NLL 损失。 